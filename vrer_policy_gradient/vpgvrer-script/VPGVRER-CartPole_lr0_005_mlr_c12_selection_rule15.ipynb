{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNkW9_pZlS9U",
        "outputId": "3cddf208-6cf4-4bf9-a4c3-bbeb33faafba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of State Space ->  4\n",
            "Size of Action Space ->  Discrete(2)\n",
            "episode:  0 score: 17.5 average score 17.5\n",
            "episode:  1 score: 25.8 average score 21.6\n",
            "episode:  2 score: 43.2 average score 28.8\n",
            "episode:  3 score: 12.2 average score 24.7\n",
            "episode:  4 score: 27.5 average score 25.2\n",
            "episode:  5 score: 20.0 average score 24.4\n",
            "episode:  6 score: 54.5 average score 28.7\n",
            "episode:  7 score: 34.0 average score 29.3\n",
            "episode:  8 score: 35.2 average score 30.0\n",
            "episode:  9 score: 42.2 average score 31.2\n",
            "episode:  10 score: 40.0 average score 32.0\n",
            "episode:  11 score: 41.5 average score 32.8\n",
            "episode:  12 score: 50.8 average score 34.2\n",
            "episode:  13 score: 37.2 average score 34.4\n",
            "episode:  14 score: 39.8 average score 34.8\n",
            "episode:  15 score: 55.5 average score 36.1\n",
            "episode:  16 score: 70.0 average score 38.1\n",
            "episode:  17 score: 63.8 average score 39.5\n",
            "episode:  18 score: 66.0 average score 40.9\n",
            "episode:  19 score: 57.5 average score 41.7\n",
            "episode:  20 score: 23.0 average score 40.8\n",
            "episode:  21 score: 40.0 average score 40.8\n",
            "episode:  22 score: 60.2 average score 41.6\n",
            "episode:  23 score: 65.5 average score 42.6\n",
            "episode:  24 score: 98.0 average score 44.8\n",
            "episode:  25 score: 50.8 average score 45.1\n",
            "episode:  26 score: 40.2 average score 44.9\n",
            "episode:  27 score: 80.0 average score 46.1\n",
            "episode:  28 score: 59.2 average score 46.6\n",
            "episode:  29 score: 41.0 average score 46.4\n",
            "episode:  30 score: 64.2 average score 47.0\n",
            "episode:  31 score: 129.0 average score 49.5\n",
            "episode:  32 score: 105.8 average score 51.2\n",
            "episode:  33 score: 123.8 average score 53.4\n",
            "episode:  34 score: 138.8 average score 55.8\n",
            "episode:  35 score: 110.8 average score 57.3\n",
            "episode:  36 score: 140.0 average score 59.6\n",
            "episode:  37 score: 140.8 average score 61.7\n",
            "episode:  38 score: 181.0 average score 64.8\n",
            "episode:  39 score: 162.2 average score 67.2\n",
            "episode:  40 score: 200.0 average score 70.5\n",
            "episode:  41 score: 185.2 average score 73.2\n",
            "episode:  42 score: 192.2 average score 76.0\n",
            "episode:  43 score: 187.2 average score 78.5\n",
            "episode:  44 score: 136.5 average score 79.8\n",
            "episode:  45 score: 176.5 average score 81.9\n",
            "episode:  46 score: 157.0 average score 83.5\n",
            "episode:  47 score: 180.2 average score 85.5\n",
            "episode:  48 score: 193.5 average score 87.7\n",
            "episode:  49 score: 200.0 average score 89.9\n",
            "episode:  50 score: 200.0 average score 92.1\n",
            "episode:  51 score: 179.0 average score 93.8\n",
            "episode:  52 score: 158.0 average score 95.0\n",
            "episode:  53 score: 183.2 average score 96.6\n",
            "episode:  54 score: 186.5 average score 98.2\n",
            "episode:  55 score: 191.2 average score 99.9\n",
            "episode:  56 score: 200.0 average score 101.7\n",
            "episode:  57 score: 162.5 average score 102.7\n",
            "episode:  58 score: 171.5 average score 103.9\n",
            "episode:  59 score: 194.8 average score 105.4\n",
            "episode:  60 score: 182.8 average score 106.7\n",
            "episode:  61 score: 185.0 average score 107.9\n",
            "episode:  62 score: 188.0 average score 109.2\n",
            "episode:  63 score: 174.5 average score 110.2\n",
            "episode:  64 score: 200.0 average score 111.6\n",
            "episode:  65 score: 200.0 average score 112.9\n",
            "episode:  66 score: 183.5 average score 114.0\n",
            "episode:  67 score: 200.0 average score 115.3\n",
            "episode:  68 score: 200.0 average score 116.5\n",
            "episode:  69 score: 172.2 average score 117.3\n",
            "episode:  70 score: 183.2 average score 118.2\n",
            "episode:  71 score: 190.8 average score 119.2\n",
            "episode:  72 score: 163.5 average score 119.8\n",
            "episode:  73 score: 188.2 average score 120.8\n",
            "episode:  74 score: 169.5 average score 121.4\n",
            "episode:  75 score: 200.0 average score 122.4\n",
            "episode:  76 score: 189.2 average score 123.3\n",
            "episode:  77 score: 200.0 average score 124.3\n",
            "episode:  78 score: 189.2 average score 125.1\n",
            "episode:  79 score: 163.2 average score 125.6\n",
            "episode:  80 score: 189.8 average score 126.4\n",
            "episode:  81 score: 197.0 average score 127.2\n",
            "episode:  82 score: 199.2 average score 128.1\n",
            "episode:  83 score: 200.0 average score 129.0\n",
            "episode:  84 score: 177.0 average score 129.5\n",
            "episode:  85 score: 200.0 average score 130.3\n",
            "episode:  86 score: 176.5 average score 130.9\n",
            "episode:  87 score: 200.0 average score 131.7\n",
            "episode:  88 score: 173.8 average score 132.1\n",
            "episode:  89 score: 188.8 average score 132.8\n",
            "episode:  90 score: 200.0 average score 133.5\n",
            "episode:  91 score: 153.8 average score 133.7\n",
            "episode:  92 score: 200.0 average score 134.4\n",
            "episode:  93 score: 192.5 average score 135.1\n",
            "episode:  94 score: 165.2 average score 135.4\n",
            "episode:  95 score: 196.2 average score 136.0\n",
            "episode:  96 score: 200.0 average score 136.7\n",
            "episode:  97 score: 200.0 average score 137.3\n",
            "episode:  98 score: 193.8 average score 137.9\n",
            "episode:  99 score: 200.0 average score 138.5\n",
            "episode:  100 score: 197.0 average score 140.3\n",
            "episode:  101 score: 200.0 average score 142.0\n",
            "episode:  102 score: 200.0 average score 143.6\n",
            "episode:  103 score: 163.5 average score 145.1\n",
            "episode:  104 score: 200.0 average score 146.8\n",
            "episode:  105 score: 151.2 average score 148.2\n",
            "episode:  106 score: 193.5 average score 149.5\n",
            "episode:  107 score: 163.2 average score 150.8\n",
            "episode:  108 score: 197.2 average score 152.5\n",
            "episode:  109 score: 200.0 average score 154.0\n",
            "episode:  110 score: 200.0 average score 155.6\n",
            "episode:  111 score: 175.5 average score 157.0\n",
            "episode:  112 score: 200.0 average score 158.5\n",
            "episode:  113 score: 200.0 average score 160.1\n",
            "episode:  114 score: 200.0 average score 161.7\n",
            "episode:  115 score: 200.0 average score 163.1\n",
            "episode:  116 score: 181.0 average score 164.3\n",
            "episode:  117 score: 200.0 average score 165.6\n",
            "episode:  118 score: 200.0 average score 167.0\n",
            "episode:  119 score: 200.0 average score 168.4\n",
            "episode:  120 score: 200.0 average score 170.2\n",
            "episode:  121 score: 200.0 average score 171.8\n",
            "episode:  122 score: 200.0 average score 173.2\n",
            "episode:  123 score: 200.0 average score 174.5\n",
            "episode:  124 score: 200.0 average score 175.5\n",
            "episode:  125 score: 200.0 average score 177.0\n",
            "episode:  126 score: 200.0 average score 178.6\n",
            "episode:  127 score: 200.0 average score 179.8\n",
            "episode:  128 score: 200.0 average score 181.2\n",
            "episode:  129 score: 200.0 average score 182.8\n",
            "episode:  130 score: 189.0 average score 184.1\n",
            "episode:  131 score: 200.0 average score 184.8\n",
            "episode:  132 score: 200.0 average score 185.7\n",
            "episode:  133 score: 200.0 average score 186.5\n",
            "episode:  134 score: 200.0 average score 187.1\n",
            "episode:  135 score: 200.0 average score 188.0\n",
            "episode:  136 score: 200.0 average score 188.6\n",
            "episode:  137 score: 200.0 average score 189.2\n",
            "episode:  138 score: 196.8 average score 189.3\n",
            "episode:  139 score: 189.8 average score 189.6\n",
            "episode:  140 score: 194.5 average score 189.5\n",
            "episode:  141 score: 200.0 average score 189.7\n",
            "episode:  142 score: 189.8 average score 189.7\n",
            "episode:  143 score: 200.0 average score 189.8\n",
            "episode:  144 score: 200.0 average score 190.4\n",
            "episode:  145 score: 200.0 average score 190.7\n",
            "episode:  146 score: 200.0 average score 191.1\n",
            "episode:  147 score: 152.5 average score 190.8\n",
            "episode:  148 score: 180.0 average score 190.7\n",
            "episode:  149 score: 200.0 average score 190.7\n",
            "episode:  150 score: 200.0 average score 190.7\n",
            "episode:  151 score: 200.0 average score 190.9\n",
            "episode:  152 score: 200.0 average score 191.3\n",
            "episode:  153 score: 200.0 average score 191.5\n",
            "episode:  154 score: 200.0 average score 191.6\n",
            "episode:  155 score: 200.0 average score 191.7\n",
            "episode:  156 score: 200.0 average score 191.7\n",
            "episode:  157 score: 200.0 average score 192.1\n",
            "episode:  158 score: 200.0 average score 192.4\n",
            "episode:  159 score: 200.0 average score 192.4\n",
            "episode:  160 score: 200.0 average score 192.6\n",
            "episode:  161 score: 200.0 average score 192.7\n",
            "episode:  162 score: 189.8 average score 192.8\n",
            "episode:  163 score: 200.0 average score 193.0\n",
            "episode:  164 score: 200.0 average score 193.0\n",
            "episode:  165 score: 200.0 average score 193.0\n",
            "episode:  166 score: 200.0 average score 193.2\n",
            "episode:  167 score: 200.0 average score 193.2\n",
            "episode:  168 score: 200.0 average score 193.2\n",
            "episode:  169 score: 200.0 average score 193.4\n",
            "episode:  170 score: 200.0 average score 193.6\n",
            "episode:  171 score: 200.0 average score 193.7\n",
            "episode:  172 score: 200.0 average score 194.1\n",
            "episode:  173 score: 184.2 average score 194.0\n",
            "episode:  174 score: 200.0 average score 194.3\n",
            "episode:  175 score: 182.8 average score 194.2\n",
            "episode:  176 score: 194.2 average score 194.2\n",
            "episode:  177 score: 200.0 average score 194.2\n",
            "episode:  178 score: 200.0 average score 194.3\n",
            "episode:  179 score: 200.0 average score 194.7\n",
            "episode:  180 score: 200.0 average score 194.8\n",
            "episode:  181 score: 200.0 average score 194.8\n",
            "episode:  182 score: 200.0 average score 194.8\n",
            "episode:  183 score: 200.0 average score 194.8\n",
            "episode:  184 score: 200.0 average score 195.1\n",
            "Solved at episode 184!\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "# from reinforce_tf2 import Agent\n",
        "# from utils import plotLearning\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "# import pymc3 as pm\n",
        "from scipy.stats import beta\n",
        "import scipy\n",
        "import gc\n",
        "\n",
        "class PolicyGradientNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims=32, fc2_dims=32):\n",
        "        super(PolicyGradientNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.pi = Dense(n_actions, activation='softmax')\n",
        "    def call(self, state):\n",
        "        value = self.fc1(state)\n",
        "        value = self.fc2(value)\n",
        "        pi = self.pi(value)\n",
        "        return pi\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.003, gamma=0.99, n_actions=4, n_k=4, num_episodes=2000, layer1_size=256, layer2_size=256):\n",
        "        self.c = 1.2\n",
        "        self.gamma = gamma\n",
        "        self.lr = alpha\n",
        "        self.n_actions = n_actions\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.state_memory_full = []\n",
        "        self.action_memory_full = []\n",
        "        self.reward_memory_full = []\n",
        "        self.G_memory_full = []\n",
        "        self.state_memory = []\n",
        "        self.model_memory = []\n",
        "        self.reuses = []\n",
        "        self.variance = []\n",
        "        self.time_elapsed = []\n",
        "        self.gradient_norm = []\n",
        "        self.loglikelihoods = np.zeros((num_episodes, n_k, num_episodes))\n",
        "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
        "        # self.policy.compile(optimizer=SGD(learning_rate=self.lr, decay=0.0))\n",
        "        self.policy.compile(optimizer=Adam(learning_rate=self.lr))\n",
        "        self._policy_hist = PolicyGradientNetwork(n_actions=n_actions)\n",
        "        # self._policy_hist.compile(optimizer=SGD(learning_rate=self.lr))\n",
        "    def choose_action(self, observation):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        probs = self.policy(state)\n",
        "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "        action = action_probs.sample()\n",
        "        return action.numpy()[0]\n",
        "    def store_transition(self, observation, action, reward):\n",
        "        # (iter, r, H)\n",
        "        self.state_memory = observation\n",
        "        self.action_memory = action\n",
        "        self.reward_memory = reward\n",
        "    def compute_ilr(self):\n",
        "        return\n",
        "    def gradient_compute(self, model, i, j, p, n_k, H):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            cur_likelihood = 0\n",
        "            loss = 0\n",
        "            for idx, (g, state) in enumerate(zip(self.G_memory_full[i][j][:], self.state_memory_full[i][j])):\n",
        "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "                if model == None:\n",
        "                    model = self._policy_hist.set_weights(self.model_memory[p])\n",
        "                probs = model(state)\n",
        "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "                log_prob = action_probs.log_prob(self.action_memory_full[i][j][idx])\n",
        "                # loss[j, idx] = -g * tf.squeeze(log_prob)\n",
        "                loss = -g * tf.squeeze(log_prob)\n",
        "                cur_likelihood += tf.squeeze(log_prob)\n",
        "        return tape.gradient(loss, model.trainable_variables), cur_likelihood\n",
        "    def mixture_gradient_compute(self, reuse, n_k, num_iters):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            loss = 0\n",
        "            for i in reuse:\n",
        "                for j in range(n_k):\n",
        "                    numerator = np.exp(self.loglikelihoods[i, j, num_iters - 1])\n",
        "                    reuse_mixture = [k for k in reuse if k >= i]\n",
        "                    denominator = np.sum(np.exp(self.loglikelihoods[i, j, [k for k in reuse if k >= i]])) / len(reuse_mixture)\n",
        "                    for idx, (g, state) in enumerate(zip(self.G_memory_full[i][j][:], self.state_memory_full[i][j])):\n",
        "                        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "                        probs = self.policy(state)\n",
        "                        action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "                        log_prob = action_probs.log_prob(self.action_memory_full[i][j][idx])\n",
        "                        loss += - numerator/denominator * g * tf.squeeze(log_prob)\n",
        "            loss = loss / (len(reuse) * n_k)\n",
        "        return tape.gradient(loss, self.policy.trainable_variables)\n",
        "    def learn(self):\n",
        "        # actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
        "        # rewards = np.array(self.reward_memory)\n",
        "        n_k = len(self.reward_memory)\n",
        "        G = {}\n",
        "        for j in range(n_k):\n",
        "            rewards = self.reward_memory[j]\n",
        "            H = len(self.reward_memory[j])\n",
        "            G_j = np.zeros_like(rewards)\n",
        "            for t in range(H):\n",
        "                G_sum = 0\n",
        "                discount = 1\n",
        "                for k in range(t, H):\n",
        "                    G_sum += rewards[k] * discount\n",
        "                    discount *= self.gamma\n",
        "                G_j[t] = G_sum\n",
        "            G[j] = G_j\n",
        "        # store cur info to full\n",
        "        self.G_memory_full.append(G)\n",
        "        self.state_memory_full.append(self.state_memory)\n",
        "        self.action_memory_full.append(self.action_memory)\n",
        "        self.reward_memory_full.append(self.reward_memory)\n",
        "        num_iters = len(self.reward_memory_full)\n",
        "        # loss = np.zeros((n_k, H), dtype = 'float32') # tf.zeros((n_k, H))\n",
        "        # cur_likelihood = np.zeros((n_k, H))\n",
        "        grad_agg = []\n",
        "        timer1 = time.time()\n",
        "        for j in range(n_k):\n",
        "            grad, ll = self.gradient_compute(self.policy, -1, j, -1, n_k, H)\n",
        "            grad_numpy = [g.numpy().flatten() for g in grad]\n",
        "            grad_numpy = np.concatenate(grad_numpy)\n",
        "            grad_agg.append(grad_numpy)\n",
        "            self.loglikelihoods[num_iters - 1, j, num_iters - 1] = ll\n",
        "        policy_param_size = len(grad_numpy)\n",
        "        cur_pg_variance = np.stack(grad_agg, axis=0)\n",
        "        # self.gradient_norm.append(np.mean(np.linalg.norm(cur_pg_variance, ord=1, axis=1)))\n",
        "        cur_pg_variance = np.mean(np.linalg.norm(cur_pg_variance, ord=2, axis=1))\n",
        "        self.variance.append(cur_pg_variance)\n",
        "        # compute the nested likelihood ratio\n",
        "        timer2 = time.time()\n",
        "        gradient = np.zeros((num_iters, n_k, policy_param_size))\n",
        "        for j in range(n_k):\n",
        "            gradient[num_iters - 1, j, :] = grad_agg[j]\n",
        "        # i-th iter\n",
        "        for i in range(num_iters):\n",
        "            # j-th replicate data\n",
        "            for j in range(n_k):\n",
        "                grad, ll = self.gradient_compute(self.policy, i, j, -1, n_k, H)\n",
        "                grad_numpy = [g.numpy().flatten() for g in grad]\n",
        "                grad_numpy = np.concatenate(grad_numpy)\n",
        "                gradient[i, j, :] = grad_numpy\n",
        "                self.loglikelihoods[i, j, num_iters-1] = ll\n",
        "        loss_ilr_i_j = np.zeros((n_k, policy_param_size))\n",
        "        reuse_iter = []\n",
        "        for i in range(num_iters):\n",
        "            for j in range(n_k):\n",
        "                numerator = np.exp(self.loglikelihoods[i,j,num_iters-1])\n",
        "                denominator = np.exp(self.loglikelihoods[i,j,i])\n",
        "                loss_ilr_i_j[j, :] = numerator / denominator * gradient[i, j, :]\n",
        "            cur_ilr_variance = np.mean(np.linalg.norm(loss_ilr_i_j, ord=2, axis=1)) # ith\n",
        "            if cur_ilr_variance <= self.c * cur_pg_variance:\n",
        "                reuse_iter.append(i)\n",
        "        timer3 = time.time()\n",
        "        self.reuses.append(reuse_iter)\n",
        "        gradient = self.mixture_gradient_compute(reuse_iter, n_k, num_iters)\n",
        "        ## \n",
        "        grad_numpy = [g.numpy().flatten() for g in gradient]\n",
        "        grad_numpy = np.concatenate(grad_numpy)\n",
        "        grad_agg.append(grad_numpy)\n",
        "        cur_pg_variance = np.stack(grad_agg, axis=0)\n",
        "        self.gradient_norm.append(np.mean(np.linalg.norm(cur_pg_variance, ord=2, axis=1)))\n",
        "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
        "        timer4 = time.time()\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.time_elapsed.append([timer2 - timer1, timer3 -timer2, timer4 - timer3])\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    index = 33\n",
        "    seed = 2021 + index\n",
        "    n_k = 4\n",
        "    path = \"/content/drive/MyDrive/Cartpole/seed-{}-n_k-{}-id-{}-c-{}\".format(seed, n_k, index, 1.2)\n",
        "    num_episodes = 2000 # iteraction\n",
        "    problem = \"CartPole-v0\" # \"LunarLander-v2\"\n",
        "    env = gym.make(problem)\n",
        "    # env._max_episode_steps = 200\n",
        "    num_states = env.observation_space.shape[0]\n",
        "    print(\"Size of State Space ->  {}\".format(num_states))\n",
        "    num_actions = env.action_space\n",
        "    print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    agent = Agent(alpha=0.01, gamma=0.99, n_actions=2, n_k=4, num_episodes=500)\n",
        "    score_history = []\n",
        "    for i in range(num_episodes):\n",
        "        score = 0\n",
        "        old_weights = agent.policy.get_weights()\n",
        "        model = [old_weights]\n",
        "        observations = {}\n",
        "        actions = {}\n",
        "        rewards = {}\n",
        "        for j in range(n_k):\n",
        "            observations[j] = []\n",
        "            actions[j] = []\n",
        "            rewards[j] = []\n",
        "            done = False\n",
        "            observation = env.reset()\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                observations[j].append(observation)\n",
        "                actions[j].append(action)\n",
        "                rewards[j].append(reward)\n",
        "                observation = observation_\n",
        "                score += reward\n",
        "            # print(rewards)\n",
        "        agent.store_transition(observations, actions, rewards)\n",
        "        score_history.append(score / n_k)\n",
        "        # old_weights = agent.policy.get_weights()\n",
        "        # agent.model_memory.append(old_weights)\n",
        "        agent.learn()\n",
        "        agent.policy.save_weights(path + \"/model-{}\".format(i))\n",
        "        # Update running reward to check condition for solving\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "        print('episode: ', i,'score: %.1f' % (score / n_k),\n",
        "            'average score %.1f' % avg_score)\n",
        "        # template = \"reuse window: {}\"\n",
        "        # print(template.format(agent.reuses[-1]))\n",
        "        if avg_score >= 195:  # Condition to consider the task solved\n",
        "            print(\"Solved at episode {}!\".format(i))\n",
        "            break\n",
        "    with open(path+'/reuses.txt', 'w') as f:\n",
        "        for _list in agent.reuses:\n",
        "            for i in range(len(_list)):\n",
        "                #f.seek(0)\n",
        "                if i == len(_list) - 1:\n",
        "                    f.write(str(_list[i]) + '\\n')\n",
        "                else:\n",
        "                    f.write(str(_list[i]) + ',')\n",
        "    np.save(path+'/variance', agent.variance)\n",
        "    np.save(path+'/time_elapsed', agent.time_elapsed)\n",
        "    np.save(path+\"/score_history\",score_history)\n",
        "    np.save(path+\"/gradient_norm\",agent.gradient_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tr3DEpvBVxwX"
      },
      "outputs": [],
      "source": [
        "# episode:  0 score: 40.5 average score 40.5\n",
        "# episode:  1 score: 18.0 average score 29.2\n",
        "# episode:  2 score: 33.2 average score 30.6\n",
        "# episode:  3 score: 17.0 average score 27.2\n",
        "# episode:  4 score: 24.5 average score 26.6\n",
        "# episode:  5 score: 32.0 average score 27.5\n",
        "# episode:  6 score: 31.0 average score 28.0\n",
        "# episode:  7 score: 30.5 average score 28.3\n",
        "# episode:  8 score: 36.5 average score 29.2\n",
        "# episode:  9 score: 36.5 average score 30.0\n",
        "# episode:  10 score: 22.8 average score 29.3\n",
        "# episode:  11 score: 31.5 average score 29.5\n",
        "# episode:  12 score: 32.2 average score 29.7\n",
        "# episode:  13 score: 38.0 average score 30.3\n",
        "# episode:  14 score: 28.8 average score 30.2\n",
        "# episode:  15 score: 40.5 average score 30.8\n",
        "# episode:  16 score: 33.2 average score 31.0\n",
        "# episode:  17 score: 52.5 average score 32.2\n",
        "# episode:  18 score: 35.0 average score 32.3\n",
        "# episode:  19 score: 28.8 average score 32.1\n",
        "# episode:  20 score: 47.8 average score 32.9\n",
        "# episode:  21 score: 19.5 average score 32.3\n",
        "# episode:  22 score: 41.5 average score 32.7\n",
        "# episode:  23 score: 39.2 average score 33.0\n",
        "# episode:  24 score: 56.8 average score 33.9\n",
        "# episode:  25 score: 29.8 average score 33.8\n",
        "# episode:  26 score: 44.8 average score 34.2\n",
        "# episode:  27 score: 58.2 average score 35.0\n",
        "# episode:  28 score: 63.2 average score 36.0\n",
        "# episode:  29 score: 41.2 average score 36.2\n",
        "# episode:  30 score: 46.0 average score 36.5\n",
        "# episode:  31 score: 32.5 average score 36.4\n",
        "# episode:  32 score: 63.0 average score 37.2\n",
        "# episode:  33 score: 47.2 average score 37.5\n",
        "# episode:  34 score: 44.2 average score 37.7\n",
        "# episode:  35 score: 64.8 average score 38.4\n",
        "# episode:  36 score: 61.8 average score 39.0\n",
        "# episode:  37 score: 84.8 average score 40.2\n",
        "# episode:  38 score: 52.5 average score 40.6\n",
        "# episode:  39 score: 76.2 average score 41.5\n",
        "# episode:  40 score: 31.5 average score 41.2\n",
        "# episode:  41 score: 35.2 average score 41.1\n",
        "# episode:  42 score: 102.8 average score 42.5\n",
        "# episode:  43 score: 61.5 average score 42.9\n",
        "# episode:  44 score: 51.8 average score 43.1\n",
        "# episode:  45 score: 117.5 average score 44.7\n",
        "# episode:  46 score: 85.5 average score 45.6\n",
        "# episode:  47 score: 88.8 average score 46.5\n",
        "# episode:  48 score: 112.2 average score 47.9\n",
        "# episode:  49 score: 64.0 average score 48.2\n",
        "# episode:  50 score: 111.5 average score 49.4\n",
        "# episode:  51 score: 98.8 average score 50.4\n",
        "# episode:  52 score: 101.0 average score 51.3\n",
        "# episode:  53 score: 153.5 average score 53.2\n",
        "# episode:  54 score: 152.2 average score 55.0\n",
        "# episode:  55 score: 104.2 average score 55.9\n",
        "# episode:  56 score: 141.5 average score 57.4\n",
        "# episode:  57 score: 115.8 average score 58.4\n",
        "# episode:  58 score: 107.0 average score 59.2\n",
        "# episode:  59 score: 134.0 average score 60.5\n",
        "# episode:  60 score: 78.5 average score 60.8\n",
        "# episode:  61 score: 186.8 average score 62.8\n",
        "# episode:  62 score: 165.8 average score 64.4\n",
        "# episode:  63 score: 162.2 average score 66.0\n",
        "# episode:  64 score: 153.8 average score 67.3\n",
        "# episode:  65 score: 168.0 average score 68.8\n",
        "# episode:  66 score: 82.0 average score 69.0\n",
        "# episode:  67 score: 130.0 average score 69.9\n",
        "# episode:  68 score: 150.2 average score 71.1\n",
        "# episode:  69 score: 150.0 average score 72.2\n",
        "# episode:  70 score: 194.0 average score 73.9\n",
        "# episode:  71 score: 159.0 average score 75.1\n",
        "# episode:  72 score: 175.8 average score 76.5\n",
        "# episode:  73 score: 169.8 average score 77.8\n",
        "# episode:  74 score: 192.8 average score 79.3\n",
        "# episode:  75 score: 161.8 average score 80.4\n",
        "# episode:  76 score: 200.0 average score 81.9\n",
        "# episode:  77 score: 174.8 average score 83.1\n",
        "# episode:  78 score: 174.2 average score 84.3\n",
        "# episode:  79 score: 119.8 average score 84.7\n",
        "# episode:  80 score: 185.2 average score 86.0\n",
        "# episode:  81 score: 192.0 average score 87.2\n",
        "# episode:  82 score: 148.5 average score 88.0\n",
        "# episode:  83 score: 151.8 average score 88.7\n",
        "# episode:  84 score: 142.2 average score 89.4\n",
        "# episode:  85 score: 135.2 average score 89.9\n",
        "# episode:  86 score: 185.5 average score 91.0\n",
        "# episode:  87 score: 191.8 average score 92.2\n",
        "# episode:  88 score: 182.5 average score 93.2\n",
        "# episode:  89 score: 200.0 average score 94.4\n",
        "# episode:  90 score: 200.0 average score 95.5\n",
        "# episode:  91 score: 200.0 average score 96.7\n",
        "# episode:  92 score: 157.5 average score 97.3\n",
        "# episode:  93 score: 179.5 average score 98.2\n",
        "# episode:  94 score: 187.2 average score 99.1\n",
        "# episode:  95 score: 172.8 average score 99.9\n",
        "# episode:  96 score: 109.5 average score 100.0\n",
        "# episode:  97 score: 155.5 average score 100.6\n",
        "# episode:  98 score: 103.5 average score 100.6\n",
        "# episode:  99 score: 159.0 average score 101.2\n",
        "# episode:  100 score: 166.5 average score 102.4\n",
        "# episode:  101 score: 120.0 average score 103.4\n",
        "# episode:  102 score: 200.0 average score 105.1\n",
        "# episode:  103 score: 200.0 average score 106.9\n",
        "# episode:  104 score: 181.8 average score 108.5\n",
        "# episode:  105 score: 187.8 average score 110.1\n",
        "# episode:  106 score: 193.5 average score 111.7\n",
        "# episode:  107 score: 200.0 average score 113.4\n",
        "# episode:  108 score: 200.0 average score 115.0\n",
        "# episode:  109 score: 200.0 average score 116.7\n",
        "# episode:  110 score: 200.0 average score 118.4\n",
        "# episode:  111 score: 200.0 average score 120.1\n",
        "# episode:  112 score: 197.0 average score 121.8\n",
        "# episode:  113 score: 191.5 average score 123.3\n",
        "# episode:  114 score: 200.0 average score 125.0\n",
        "# episode:  115 score: 200.0 average score 126.6\n",
        "# episode:  116 score: 200.0 average score 128.3\n",
        "# episode:  117 score: 158.8 average score 129.3\n",
        "# episode:  118 score: 200.0 average score 131.0\n",
        "# episode:  119 score: 200.0 average score 132.7\n",
        "# episode:  120 score: 200.0 average score 134.2\n",
        "# episode:  121 score: 200.0 average score 136.0\n",
        "# episode:  122 score: 200.0 average score 137.6\n",
        "# episode:  123 score: 194.0 average score 139.2\n",
        "# episode:  124 score: 200.0 average score 140.6\n",
        "# episode:  125 score: 200.0 average score 142.3\n",
        "# episode:  126 score: 200.0 average score 143.8\n",
        "# episode:  127 score: 200.0 average score 145.3\n",
        "# episode:  128 score: 200.0 average score 146.6\n",
        "# episode:  129 score: 195.5 average score 148.2\n",
        "# episode:  130 score: 143.2 average score 149.2\n",
        "# episode:  131 score: 182.5 average score 150.7\n",
        "# episode:  132 score: 200.0 average score 152.0\n",
        "# episode:  133 score: 190.0 average score 153.4\n",
        "# episode:  134 score: 200.0 average score 155.0\n",
        "# episode:  135 score: 194.8 average score 156.3\n",
        "# episode:  136 score: 182.5 average score 157.5\n",
        "# episode:  137 score: 180.5 average score 158.5\n",
        "# episode:  138 score: 200.0 average score 159.9\n",
        "# episode:  139 score: 200.0 average score 161.2\n",
        "# episode:  140 score: 200.0 average score 162.9\n",
        "# episode:  141 score: 200.0 average score 164.5\n",
        "# episode:  142 score: 200.0 average score 165.5\n",
        "# episode:  143 score: 193.5 average score 166.8\n",
        "# episode:  144 score: 200.0 average score 168.3\n",
        "# episode:  145 score: 163.0 average score 168.7\n",
        "# episode:  146 score: 166.8 average score 169.6\n",
        "# episode:  147 score: 194.0 average score 170.6\n",
        "# episode:  148 score: 174.8 average score 171.2\n",
        "# episode:  149 score: 163.2 average score 172.2\n",
        "# episode:  150 score: 165.0 average score 172.8\n",
        "# episode:  151 score: 171.5 average score 173.5\n",
        "# episode:  152 score: 141.8 average score 173.9\n",
        "# episode:  153 score: 176.2 average score 174.1\n",
        "# episode:  154 score: 186.0 average score 174.5\n",
        "# episode:  155 score: 195.5 average score 175.4\n",
        "# episode:  156 score: 189.0 average score 175.8\n",
        "# episode:  157 score: 173.2 average score 176.4\n",
        "# episode:  158 score: 128.0 average score 176.6\n",
        "# episode:  159 score: 150.0 average score 176.8\n",
        "# episode:  160 score: 176.2 average score 177.8\n",
        "# episode:  161 score: 150.2 average score 177.4\n",
        "# episode:  162 score: 140.2 average score 177.2\n",
        "# episode:  163 score: 179.5 average score 177.3\n",
        "# episode:  164 score: 200.0 average score 177.8\n",
        "# episode:  165 score: 200.0 average score 178.1\n",
        "# episode:  166 score: 159.5 average score 178.9\n",
        "# episode:  167 score: 183.8 average score 179.4\n",
        "# episode:  168 score: 200.0 average score 179.9\n",
        "# episode:  169 score: 200.0 average score 180.4\n",
        "# episode:  170 score: 200.0 average score 180.5\n",
        "# episode:  171 score: 200.0 average score 180.9\n",
        "# episode:  172 score: 200.0 average score 181.1\n",
        "# episode:  173 score: 200.0 average score 181.4\n",
        "# episode:  174 score: 200.0 average score 181.5\n",
        "# episode:  175 score: 200.0 average score 181.9\n",
        "# episode:  176 score: 200.0 average score 181.9\n",
        "# episode:  177 score: 200.0 average score 182.1\n",
        "# episode:  178 score: 200.0 average score 182.4\n",
        "# episode:  179 score: 195.8 average score 183.2\n",
        "# episode:  180 score: 200.0 average score 183.3\n",
        "# episode:  181 score: 200.0 average score 183.4\n",
        "# episode:  182 score: 193.5 average score 183.8\n",
        "# episode:  183 score: 140.5 average score 183.7\n",
        "# episode:  184 score: 200.0 average score 184.3\n",
        "# episode:  185 score: 156.8 average score 184.5\n",
        "# episode:  186 score: 168.2 average score 184.3\n",
        "# episode:  187 score: 200.0 average score 184.4\n",
        "# episode:  188 score: 200.0 average score 184.6\n",
        "# episode:  189 score: 188.8 average score 184.5\n",
        "# episode:  190 score: 200.0 average score 184.5\n",
        "# episode:  191 score: 160.0 average score 184.1\n",
        "# episode:  192 score: 188.5 average score 184.4\n",
        "# episode:  193 score: 200.0 average score 184.6\n",
        "# episode:  194 score: 157.2 average score 184.3\n",
        "# episode:  195 score: 189.0 average score 184.5\n",
        "# episode:  196 score: 151.8 average score 184.9\n",
        "# episode:  197 score: 103.2 average score 184.4\n",
        "# episode:  198 score: 148.0 average score 184.8\n",
        "# episode:  199 score: 177.0 average score 185.0\n",
        "# episode:  200 score: 166.5 average score 185.0\n",
        "# episode:  201 score: 172.0 average score 185.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RixZ4hefCjoi"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Cartpole/cartpole-lr0005-mlr',score_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdH0tKsSSkcT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "# from reinforce_tf2 import Agent\n",
        "# from utils import plotLearning\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "# import pymc3 as pm\n",
        "from scipy.stats import beta\n",
        "import scipy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class PolicyGradientNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims=32, fc2_dims=32):\n",
        "        super(PolicyGradientNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.pi = Dense(n_actions, activation='softmax')\n",
        "    def call(self, state):\n",
        "        value = self.fc1(state)\n",
        "        value = self.fc2(value)\n",
        "        pi = self.pi(value)\n",
        "        return pi\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.003, gamma=0.99, n_actions=4, n_k=4, num_episodes=2000, layer1_size=256, layer2_size=256):\n",
        "        self.c = 5\n",
        "        self.gamma = gamma\n",
        "        self.lr = alpha\n",
        "        self.n_actions = n_actions\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.state_memory_full = []\n",
        "        self.action_memory_full = []\n",
        "        self.reward_memory_full = []\n",
        "        self.G_memory_full = []\n",
        "        self.state_memory = []\n",
        "        self.model_memory = []\n",
        "        self.reuses = []\n",
        "        self.variance = []\n",
        "        self.time_elapsed = []\n",
        "        self.gradient_norm = []\n",
        "        self.loglikelihoods = np.zeros((num_episodes, n_k, num_episodes))\n",
        "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
        "        # self.policy.compile(optimizer=SGD(learning_rate=self.lr, decay=0.0))\n",
        "        self.policy.compile(optimizer=Adam(learning_rate=self.lr))\n",
        "        self._policy_hist = PolicyGradientNetwork(n_actions=n_actions)\n",
        "        # self._policy_hist.compile(optimizer=SGD(learning_rate=self.lr))\n",
        "    def choose_action(self, observation):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        probs = self.policy(state)\n",
        "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "        action = action_probs.sample()\n",
        "        return action.numpy()[0]\n",
        "    def store_transition(self, observation, action, reward):\n",
        "        # (iter, r, H)\n",
        "        self.state_memory = observation\n",
        "        self.action_memory = action\n",
        "        self.reward_memory = reward\n",
        "    def compute_ilr(self):\n",
        "        return\n",
        "    def gradient_compute(self, model, i, j, p, n_k, H):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            cur_likelihood = 0\n",
        "            loss = 0\n",
        "            for idx, (g, state) in enumerate(zip(self.G_memory_full[i][j][:], self.state_memory_full[i][j])):\n",
        "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "                if model == None:\n",
        "                    model = self._policy_hist.set_weights(self.model_memory[p])\n",
        "                probs = model(state)\n",
        "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "                log_prob = action_probs.log_prob(self.action_memory_full[i][j][idx])\n",
        "                # loss[j, idx] = -g * tf.squeeze(log_prob)\n",
        "                loss = -g * tf.squeeze(log_prob)\n",
        "                cur_likelihood += tf.squeeze(log_prob)\n",
        "        return tape.gradient(loss, model.trainable_variables), cur_likelihood\n",
        "    def mixture_gradient_compute(self, reuse, n_k, num_iters):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            loss = 0\n",
        "            for i in reuse:\n",
        "                for j in range(n_k):\n",
        "                    numerator = np.exp(self.loglikelihoods[i, j, num_iters - 1])\n",
        "                    reuse_mixture = [k for k in reuse if k >= i]\n",
        "                    denominator = np.sum(np.exp(self.loglikelihoods[i, j, [k for k in reuse if k >= i]])) / len(reuse_mixture)\n",
        "                    for idx, (g, state) in enumerate(zip(self.G_memory_full[i][j][:], self.state_memory_full[i][j])):\n",
        "                        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "                        probs = self.policy(state)\n",
        "                        action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "                        log_prob = action_probs.log_prob(self.action_memory_full[i][j][idx])\n",
        "                        loss += - numerator/denominator * g * tf.squeeze(log_prob)\n",
        "            loss = loss / (len(reuse) * n_k)\n",
        "        return tape.gradient(loss, self.policy.trainable_variables)\n",
        "    def learn(self):\n",
        "        # actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
        "        # rewards = np.array(self.reward_memory)\n",
        "        n_k = len(self.reward_memory)\n",
        "        G = {}\n",
        "        for j in range(n_k):\n",
        "            rewards = self.reward_memory[j]\n",
        "            H = len(self.reward_memory[j])\n",
        "            G_j = np.zeros_like(rewards)\n",
        "            for t in range(H):\n",
        "                G_sum = 0\n",
        "                discount = 1\n",
        "                for k in range(t, H):\n",
        "                    G_sum += rewards[k] * discount\n",
        "                    discount *= self.gamma\n",
        "                G_j[t] = G_sum\n",
        "            G[j] = G_j\n",
        "        # store cur info to full\n",
        "        self.G_memory_full.append(G)\n",
        "        self.state_memory_full.append(self.state_memory)\n",
        "        self.action_memory_full.append(self.action_memory)\n",
        "        self.reward_memory_full.append(self.reward_memory)\n",
        "        num_iters = len(self.reward_memory_full)\n",
        "        # loss = np.zeros((n_k, H), dtype = 'float32') # tf.zeros((n_k, H))\n",
        "        # cur_likelihood = np.zeros((n_k, H))\n",
        "        grad_agg = []\n",
        "        timer1 = time.time()\n",
        "        for j in range(n_k):\n",
        "            grad, ll = self.gradient_compute(self.policy, -1, j, -1, n_k, H)\n",
        "            grad_numpy = [g.numpy().flatten() for g in grad]\n",
        "            grad_numpy = np.concatenate(grad_numpy)\n",
        "            grad_agg.append(grad_numpy)\n",
        "            self.loglikelihoods[num_iters - 1, j, num_iters - 1] = ll\n",
        "        policy_param_size = len(grad_numpy)\n",
        "        cur_pg_variance = np.stack(grad_agg, axis=0)\n",
        "        # self.gradient_norm.append(np.mean(np.linalg.norm(cur_pg_variance, ord=1, axis=1)))\n",
        "        cur_pg_variance = np.mean(np.linalg.norm(cur_pg_variance, ord=2, axis=1))\n",
        "        self.variance.append(cur_pg_variance)\n",
        "        # compute the nested likelihood ratio\n",
        "        timer2 = time.time()\n",
        "        gradient = np.zeros((num_iters, n_k, policy_param_size))\n",
        "        for j in range(n_k):\n",
        "            gradient[num_iters - 1, j, :] = grad_agg[j]\n",
        "        # i-th iter\n",
        "        for i in range(num_iters):\n",
        "            # j-th replicate data\n",
        "            for j in range(n_k):\n",
        "                grad, ll = self.gradient_compute(self.policy, i, j, -1, n_k, H)\n",
        "                grad_numpy = [g.numpy().flatten() for g in grad]\n",
        "                grad_numpy = np.concatenate(grad_numpy)\n",
        "                gradient[i, j, :] = grad_numpy\n",
        "                self.loglikelihoods[i, j, num_iters-1] = ll\n",
        "        loss_ilr_i_j = np.zeros((n_k, policy_param_size))\n",
        "        reuse_iter = []\n",
        "        for i in range(num_iters):\n",
        "            for j in range(n_k):\n",
        "                numerator = np.exp(self.loglikelihoods[i,j,num_iters-1])\n",
        "                denominator = np.exp(self.loglikelihoods[i,j,i])\n",
        "                loss_ilr_i_j[j, :] = numerator / denominator * gradient[i, j, :]\n",
        "            cur_ilr_variance = np.mean(np.linalg.norm(loss_ilr_i_j, ord=2, axis=1)) # ith\n",
        "            if cur_ilr_variance <= self.c * cur_pg_variance:\n",
        "                reuse_iter.append(i)\n",
        "        timer3 = time.time()\n",
        "        self.reuses.append(reuse_iter)\n",
        "        gradient = self.mixture_gradient_compute(reuse_iter, n_k, num_iters)\n",
        "        ## \n",
        "        grad_numpy = [g.numpy().flatten() for g in gradient]\n",
        "        grad_numpy = np.concatenate(grad_numpy)\n",
        "        grad_agg.append(grad_numpy)\n",
        "        cur_pg_variance = np.stack(grad_agg, axis=0)\n",
        "        self.gradient_norm.append(np.mean(np.linalg.norm(cur_pg_variance, ord=2, axis=1)))\n",
        "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
        "        timer4 = time.time()\n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        # self.time_elapsed.append([timer2 - timer1, timer3 -timer2, timer4 - timer3])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    index = 2\n",
        "    c=5\n",
        "    seed = 2021 + index\n",
        "    n_k = 4\n",
        "    path = \"seed-{}-n_k-{}-id-{}-c{}\".format(seed, n_k, index,c)\n",
        "    num_episodes = 2000 # iteraction\n",
        "    problem = \"CartPole-v0\" # \"LunarLander-v2\"\n",
        "    env = gym.make(problem)\n",
        "    # env._max_episode_steps = 200\n",
        "    num_states = env.observation_space.shape[0]\n",
        "    print(\"Size of State Space ->  {}\".format(num_states))\n",
        "    num_actions = env.action_space\n",
        "    print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    agent = Agent(alpha=0.005, gamma=0.99, n_actions=2, n_k=4, num_episodes=2000)\n",
        "    score_history = []\n",
        "    for i in range(num_episodes):\n",
        "        score = 0\n",
        "        old_weights = agent.policy.get_weights()\n",
        "        model = [old_weights]\n",
        "        observations = {}\n",
        "        actions = {}\n",
        "        rewards = {}\n",
        "        for j in range(n_k):\n",
        "            observations[j] = []\n",
        "            actions[j] = []\n",
        "            rewards[j] = []\n",
        "            done = False\n",
        "            observation = env.reset()\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                observations[j].append(observation)\n",
        "                actions[j].append(action)\n",
        "                rewards[j].append(reward)\n",
        "                observation = observation_\n",
        "                score += reward\n",
        "            # print(rewards)\n",
        "        agent.store_transition(observations, actions, rewards)\n",
        "        score_history.append(score / n_k)\n",
        "        # old_weights = agent.policy.get_weights()\n",
        "        # agent.model_memory.append(old_weights)\n",
        "        agent.learn()\n",
        "        # agent.policy.save_weights(path + \"/model-{}\".format(i))\n",
        "        # Update running reward to check condition for solving\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "        print('episode: ', i,'score: %.1f' % (score / n_k),\n",
        "            'average score %.1f' % avg_score)\n",
        "        # template = \"reuse window: {}\"\n",
        "        # print(template.format(agent.reuses[-1]))\n",
        "        if avg_score > 195:  # Condition to consider the task solved\n",
        "            print(\"Solved at episode {}!\".format(i))\n",
        "            break\n",
        "    with open(path+'/reuses.txt', 'w') as f:\n",
        "        for _list in agent.reuses:\n",
        "            for i in range(len(_list)):\n",
        "                #f.seek(0)\n",
        "                if i == len(_list) - 1:\n",
        "                    f.write(str(_list[i]) + '\\n')\n",
        "                else:\n",
        "                    f.write(str(_list[i]) + ',')\n",
        "    np.save(path+'/variance', agent.variance)\n",
        "    np.save(path+'/time_elapsed', agent.time_elapsed)\n",
        "    np.save(path+\"/score_history\",score_history)\n",
        "    np.save(path+\"/gradient_norm\",agent.gradient_norm)\n",
        "    iter_reuse_len = {}\n",
        "    for i in agent.reuses:\n",
        "      for j in range(50):\n",
        "        if j not in iter_reuse_len:\n",
        "          iter_reuse_len[j] = [len(i[j])]\n",
        "        else:\n",
        "          iter_reuse_len[j].append(len(i[j]))\n",
        "    pd.DataFrame(iter_reuse_len).to_csv('reuse.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Copy of cartpole-lr0.005-mlr-c12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}